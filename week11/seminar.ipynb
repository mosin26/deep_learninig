{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [sample solution that works]\n",
    "\n",
    "# This tutorial is will bring you through your first deep reinforcement learning model\n",
    "\n",
    "\n",
    "* Seaquest game as an example\n",
    "* Training a simple lasagne neural network for Q_learning objective\n",
    "\n",
    "\n",
    "## About OpenAI Gym\n",
    "\n",
    "* Its a recently published platform that basicly allows you to train agents in a wide variety of environments with near-identical interface.\n",
    "* This is twice as awesome since now we don't need to write a new wrapper for every game\n",
    "* Go check it out!\n",
    "  * Blog post - https://openai.com/blog/openai-gym-beta/\n",
    "  * Github - https://github.com/openai/gym\n",
    "\n",
    "\n",
    "## New to Lasagne and AgentNet?\n",
    "* We only require surface level knowledge of theano and lasagne, so you can just learn them as you go.\n",
    "* Alternatively, you can find Lasagne tutorials here:\n",
    " * Official mnist example: http://lasagne.readthedocs.io/en/latest/user/tutorial.html\n",
    " * From scratch: https://github.com/ddtm/dl-course/tree/master/Seminar4\n",
    " * From theano: https://github.com/craffel/Lasagne-tutorial/blob/master/examples/tutorial.ipynb\n",
    "* This is pretty much the basic tutorial for AgentNet, so it's okay not to know it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global params.\n",
    "\n",
    "#game title. full list of games = http://yavar.naddaf.name/ale/list_of_current_games.html\n",
    "GAME=\"GopherDeterministic-v3\"\n",
    "\n",
    "#number of parallel agents and batch sequence length (frames)\n",
    "N_AGENTS = 10\n",
    "SEQ_LENGTH = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#game image will be resized from (210,160) to your image_size. \n",
    "#You may want a bigger image for your homework assignment IF you want a larger NN\n",
    "IMAGE_W,IMAGE_H = IMAGE_SIZE =(105,80)\n",
    "from scipy.misc import imresize\n",
    "def preprocess(obs):\n",
    "    obs= imresize(obs,IMAGE_SIZE)\n",
    "    return obs.mean(-1)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-10 00:05:07,241] Making new env: GopherDeterministic-v3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f49f0fdb210>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM0AAAD/CAYAAABSDlLPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49f159c710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "#creating a game\n",
    "atari = gym.make(GAME)\n",
    "\n",
    "action_names = np.array(atari.get_action_meanings())\n",
    "\n",
    "obs = atari.step(0)[0]\n",
    "\n",
    "plt.imshow(preprocess(obs),interpolation='none',cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic agent setup\n",
    "Here we define a simple agent that maps game images into Qvalues using simple convolutional neural network.\n",
    "\n",
    "![scheme](https://s18.postimg.org/gbmsq6gmx/dqn_scheme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "theano.config.floatX = 'float32'\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer\n",
    "\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y)\n",
    "observation_layer = InputLayer((None,IMAGE_W,IMAGE_H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.memory import WindowAugmentation,LSTMCell\n",
    "\n",
    "#store 4-tick window in order to perceive motion of objects\n",
    "prev_window = InputLayer((None,4,IMAGE_W,IMAGE_H))\n",
    "\n",
    "#update rule for this window\n",
    "current_window = WindowAugmentation(observation_layer,prev_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import Conv2DLayer,Pool2DLayer,DenseLayer,batch_norm,dropout\n",
    "\n",
    "#main neural network body\n",
    "<build network body here>\n",
    "\n",
    "#please set this to your last layer for convenience\n",
    "last_layer = <your last layer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a layer that predicts Qvalues for all actions.\n",
    "# Just adense layer with corresponding number of units and no nonlinearity (lasagne.nonlinearity.linear)\n",
    "n_actions = atari.action_space.n\n",
    "qvalues_layer = <layer that predicts Qvalues>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To pick actions, we use an epsilon-greedy resolver (epsilon is a property)\n",
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer,name=\"e-greedy action picker\")\n",
    "\n",
    "action_layer.epsilon.set_value(np.float32(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights the regular way\n",
    "weights = <all weights of your neural network>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "agent = Agent(observation_layers=observation_layer,                    #observations\n",
    "              policy_estimators=(qvalues_layer),                       #whatever else you want to monitor\n",
    "              action_layers=action_layer,                               #actions\n",
    "              agent_states={current_window:prev_window},               #dict of memory states\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "\n",
    "pool = EnvPool(agent,GAME, N_AGENTS,preprocess_observation=preprocess) #see docs on what it's capabale of\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "_,action_log,reward_log,_,_,_  = pool.interact(7)\n",
    "\n",
    "\n",
    "print(action_names[action_log][:2])\n",
    "print(reward_log[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first sessions (this function calls interact and remembers sessions)\n",
    "pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action_layer.epsilon.set_value(0)\n",
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "from random import choice\n",
    "video_path = choice([os.path.join(\"records\",fname) \n",
    "                     for fname in os.listdir(\"records\") \n",
    "                     if fname.endswith(\".mp4\")])\n",
    "\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "* Get reference Qvalues according to Qlearning algorithm\n",
    "* Train on environment interaction sessions\n",
    " * Such sessions are sequences of observations, agent memory, actions, q-values,etc\n",
    "* Implement Q-learning loss & minimize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay\n",
    "\n",
    "_,_,_,_,qvalues_seq, = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    optimize_experience_replay=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "\n",
    "#actions, shape= [batch_i, time_tick]\n",
    "actions = replay.actions[0]\n",
    "\n",
    "#rewards [batch_i,time_tick]\n",
    "rewards = replay.rewards\n",
    "\n",
    "#session indicator (0 means session ended) [batch_i,time_tick]\n",
    "is_alive = replay.is_alive\n",
    "\n",
    "#qvalues at current tick.  shape = [batch_i,time_tick, action_id]\n",
    "qvalues = qvalues_seq\n",
    "\n",
    "#q-values at next tick. shape = [batch_i,time_tick, action_id], padded with zeros for math simplicity\n",
    "next_qvalues = T.concatenate([qvalues[:, 1:],\n",
    "                              T.zeros_like(qvalues[:,:1,:]),],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clip rewards to [-1,+1] to avoid explosion.\n",
    "rewards = <clip rewards to [-1,1] range. Alternatively, scale them or just mind your learning rates>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#compute q-values for best actions\n",
    "optimal_next_qvalues = <qvalue for optimal action. Aggregate over next_qvalues>\n",
    "\n",
    "\n",
    "gamma=0.99\n",
    "\n",
    "# target Qvalues, r + gamma*max_a' Q(s', a')\n",
    "reference_qvalues = <reference qvalues, r+gamma*Q(s_next,a_max)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.learning.helpers import get_end_indicator\n",
    "\n",
    "#indicator of session end [batch_i,time_tick]\n",
    "is_end = get_end_indicator(is_alive)\n",
    "\n",
    "#set reference qvalues at session end to just the immediate rewards\n",
    "reference_qvalues = T.switch(is_end,rewards,reference_qvalues)\n",
    "\n",
    "#consider constant\n",
    "reference_qvalues = theano.gradient.disconnected_grad(reference_qvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.learning.helpers import get_action_Qvalues\n",
    "\n",
    "#q-values for chosen actions [batch_i,time_tick]\n",
    "predicted_qvalues = get_action_Qvalues(qvalues,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2 at each tick\n",
    "elwise_mse_loss = <elementwise loss for Q-learning>\n",
    "\n",
    "#exclude last tick (zeros)\n",
    "elwise_mse_loss = T.set_subtensor(elwise_mse_loss[:,-1],0)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "loss = (elwise_mse_loss*is_alive).sum() / is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute weight updates. Replace with any optimizer you want\n",
    "updates = <optimize loss over nn weights using your favorite algorithm>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile train function\n",
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#full game rewards\n",
    "rewards = {epoch_counter:untrained_reward}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_epsilon(epoch_counter):\n",
    "    \"\"\"\n",
    "    a function which outputs current epsilon for e-greedy exploration given training iteration.\n",
    "    \"\"\"\n",
    "    <implement me!>\n",
    "    return 0.1\n",
    "\n",
    "#a visualizer\n",
    "plt.plot(np.linspace(0,50000),[get_epsilon(i) for i in np.linspace(0,50000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#the loop may take eons to finish.\n",
    "#consider interrupting early.\n",
    "for i in xrange(10**7):    \n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    current_epsilon = get_epsilon(epoch_counter)\n",
    "    action_layer.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    #train\n",
    "    pool.update(SEQ_LENGTH)\n",
    "    loss = train_step()\n",
    "    \n",
    "    if epoch_counter%10==0:\n",
    "        #average reward per game tick in current experience replay pool\n",
    "        print(\"iter=%i\\tepsilon=%.3f\\tloss=%.3f\"%(epoch_counter,current_epsilon,loss))\n",
    "        \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%500 ==0:\n",
    "        n_games = 10\n",
    "        action_layer.epsilon.set_value(0)\n",
    "        rewards[epoch_counter] = pool.evaluate( record_video=False,n_games=n_games,verbose=False)\n",
    "        print(\"Current score(mean over %i) = %.3f\"%(n_games,np.mean(rewards[epoch_counter])))\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results\n",
    " * Here we plot learning curves and sample testimonials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "t,r = zip(*sorted(rewards.items(),key=lambda k:k[0]))\n",
    "plt.plot(t,pd.ewma(np.concatenate(r),alpha=0.1))\n",
    "plt.title(\"moving average of rewards over ticks of training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action_layer.epsilon.set_value(0.05)\n",
    "rw = pool.evaluate(n_games=20,save_path=\"./records\",record_video=False)\n",
    "print(\"mean session score=%f.5\"%np.mean(rw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "#select the one you want\n",
    "video_path=\"./records/openaigym.video.0.13.video000000.mp4\"\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.utils.persistence import save,load\n",
    "save(action_layer,\"gopher.pcl\")\n",
    "#load(action_layer,\"gopher.pcl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Once you got it working,\n",
    "Try building a network that maximizes the final score\n",
    "\n",
    "* Moar lasagne stuff: convolutional layers, batch normalization, nonlinearities and so on\n",
    "* Recurrent agent memory layers, GRUMemoryLayer, etc\n",
    "* Different reinforcement learning algorithm (p.e. qlearning_n_step), other parameters\n",
    "* Experience replay pool\n",
    "\n",
    "\n",
    "Look for info?\n",
    "* [lasagne doc](http://lasagne.readthedocs.io/en/latest/)\n",
    "* [agentnet doc](http://agentnet.readthedocs.io/en/latest/)\n",
    "* [gym homepage](http://gym.openai.com/)\n",
    "\n",
    "\n",
    "You can also try to expand to a different game: \n",
    " * all OpenAI Atari games are already compatible, you only need to change GAME_TITLE\n",
    " * Other discrete action space environments are also accessible this way\n",
    " * For continuous action spaces, either discretize actions or use continuous RL algorithms (e.g. .learning.dpg_n_step)\n",
    " * Adapting to a custom non-OpenAI environment can be done with a simple wrapper\n",
    " \n",
    " \n",
    "__Good luck!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
