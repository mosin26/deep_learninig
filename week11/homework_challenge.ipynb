{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework assignment\n",
    "\n",
    "Your main assignment at this point is to train and evaluate a DQN or similar architecture on another game.\n",
    "\n",
    "For starters, we recommend one of these games:\n",
    "* seaquest\n",
    "* ms_pacman\n",
    "* gopher (get >1.5k score)\n",
    "* kung_fu_master\n",
    "* alien\n",
    "\n",
    "You can use classwork_solution notebook for reference, but know that it is __suboptimal in terms of training speed__, so you will be able to improve over it.\n",
    "\n",
    "### Formal\n",
    "A submission should contain \n",
    "* your code (it's okay to use library functions)\n",
    "* evaluation (total rewards from 20+ games in a row)\n",
    "* some videos of your DQN performance\n",
    "* An ideal submission would also contain saved weights and a [binder](mybinder.org)-ready (or similar) repository like this where one can load and run a pre-trained model.\n",
    "* please add jheuristic@yandex-team.ru when submitting\n",
    "\n",
    "Since we are using an altered version of OpenAI Gym atari (removing the known bug with flickering), submitting it directly to gym would be impossible until they change the standard environment. You can, however, pick an official environment (via gym.make) instead of modified one and fix the flickering issue on agent side.\n",
    "\n",
    "It's probably a good idea to look onto the classwork_solution.ipynb once you got through your notebook.\n",
    "\n",
    "### Tips\n",
    "\n",
    "__Fix Qlearning loss explosion__(if you get NaNs):\n",
    " * Some games have rewards on a larger scale than others.\n",
    "  * this causes mean squared error to explode\n",
    " * Donwscale or clip rewards when computing objective function\n",
    " * Alternatively, reduce learning rate or switch to a more robust optimization algorithm\n",
    "\n",
    "__Stabilize training__:\n",
    " * More parallel agents\n",
    "  * works with any method, but required more computation than experience replay\n",
    " * Experience replay (see solution)\n",
    "  * works less efficiently with on-policy RL (SARSA, A2c)\n",
    " * Target networks\n",
    "  * idea: take referece Q-values from an earlier network snapshot\n",
    "  * `from agentnet.target_network import TargetNetwork`; rtfd.\n",
    " * Google some new algorithms\n",
    "\n",
    "__Other learning algorithms__:\n",
    " * Most common algorithms already implemented in agentnet.learning.*\n",
    " * One can implement any other loss function using theano\n",
    " * Sarsa, K-step Qlearning and Advantage Actor Critic are on policy algorithms\n",
    "  * they tend to have slower convergence if you use long experience replay buffer\n",
    " * Some learning functions have a space for known modiffications\n",
    "  * e.g. qlearning supports Double Q-learning, bootstrap Q-learning and others\n",
    "\n",
    "__Recurrent memory cells__:\n",
    " * Only make sense in partially observable environments.\n",
    " * Basically a DQN trained with BPTT.\n",
    " * Now SEQ_LENGTH starts to matter\n",
    " * See agentnet docs and examples for some use cases.\n",
    " \n",
    "__Other__:\n",
    " * Avoid using SEQ_LENGTH=1: you need \"next state\" for most algorithms.\n",
    " * Qlearning objective MSE can and will raise and fall between iterations. The only thing that matters is average session reward.\n",
    " * General coding advice: only add more complexity when you're sure that current version works fine.\n",
    " * If docs didn't solve it, contact us for help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
