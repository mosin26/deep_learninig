{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Vasiliy Mosin***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * 1-dimensional convolutions for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500        0.0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.shape, df.is_blocked.mean()\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.228222107326\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print \"Blocked ratio\",df.is_blocked.mean()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.504583519879\n",
      "Count: 544996\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "import random\n",
    "random.seed(26)\n",
    "ind1 = random.sample(df[df.is_blocked==0].itemid, 270000)\n",
    "ind2 = df[df.is_blocked==1].itemid\n",
    "ind1.extend(ind2)\n",
    "df = df[df.itemid.isin(ind1)]\n",
    "\n",
    "print \"Blocked ratio:\",df.is_blocked.mean()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print \"All tests passed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In case your RAM-o-meter is in the red\n",
    "if very_low_RAM:\n",
    "    df = df[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('train.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.decode('utf8').lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAFkCAYAAAD7dJuCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDCeSJKkqhhNJklSV/w/Ba36JScVjEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5d81fbf8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_=plt.hist(token_counts.values(),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "min_count = 10\n",
    "tokens = []\n",
    "for token in token_counts.keys():\n",
    "    if token_counts.get(token) >= min_count:\n",
    "        tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 87438\n"
     ]
    }
   ],
   "source": [
    "print \"# Tokens:\",len(token_to_id)\n",
    "if len(token_to_id) < 30000:\n",
    "    print \"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\"\n",
    "if len(token_to_id) > 1000000:\n",
    "    print \"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.decode('utf8').lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = map(lambda token: token_to_id.get(token,0), tokens)[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (544996, 15)\n",
      "Костюм Didriksons Boardman, размер 100, краги, шап -> [24664 67208     0 35797 72920 24885     0     0     0     0] ...\n",
      "Поездки на таможню, печать в паспорте -> [42895 14751 55112 81746 79863 17438     0     0     0     0] ...\n",
      "Рефлекторно-урогинекологический массаж -> [ 8410     0 30427     0     0     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Размер матрицы:\",title_tokens.shape\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print title,'->', tokens[:10],'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "for cat_str, subcat_str in df[[\"category\",\"subcategory\"]].values: \n",
    "    cat_dict = {\"category\":cat_str,\"subcategory\":subcat_str}\n",
    "    categories.append(cat_dict)\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot)))\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "data_tuple = train_test_split(title_tokens,desc_tokens,df_non_text.values,target)\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading saved data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "save_prepared_data = False #save\n",
    "read_prepared_data = True #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print \"Saving preprocessed data (may take up to 3 minutes)\"\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "    print \"done\"\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print \"Reading saved data...\"\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    print \"done\"     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles\n",
    " * cnn+global max or RNN\n",
    "* Separate input for description\n",
    " * cnn+global max or RNN\n",
    "* Separate input for categorical features\n",
    " * обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp,input_size=len(token_to_id)+1,output_size=64)\n",
    "#reshape from [batch, time, unit] to [batch,unit,time] to allow 1d convolution over time\n",
    "descr_nn = lasagne.layers.DimshuffleLayer(descr_nn, [0,2,1])\n",
    "descr_nn = lasagne.layers.Conv1DLayer(descr_nn,num_filters=16,filter_size=3)\n",
    "#pool over time\n",
    "descr_nn = lasagne.layers.GlobalPoolLayer(descr_nn,T.max)\n",
    "#Possible improvements here are adding several parallel convs with different filter sizes or stacking them the usual way\n",
    "#1dconv -> 1d max pool ->1dconv and finally global pool \n",
    "\n",
    "# Titles\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp,input_size=len(token_to_id)+1,output_size=8)\n",
    "title_nn = lasagne.layers.DimshuffleLayer(title_nn, [0,2,1])\n",
    "title_nn = lasagne.layers.Conv1DLayer(title_nn,num_filters=4,filter_size=2)\n",
    "title_nn = lasagne.layers.GlobalPoolLayer(title_nn,T.max)\n",
    "\n",
    "# Non-sequences\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, num_units=32,nonlinearity=lasagne.nonlinearities.rectify)\n",
    "cat_nn = lasagne.layers.DropoutLayer(cat_nn, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.concat([descr_nn, title_nn, cat_nn])                                \n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn,1024)\n",
    "nn = lasagne.layers.DropoutLayer(nn,p=0.05)\n",
    "nn = lasagne.layers.DenseLayer(nn,1,nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction,target_y,delta = 1.0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.adam(loss,weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction,target_y,delta = 1.0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1 / 50\n",
      "\tloss: 43845.4667126\n",
      "\tacc: 0.597227722772\n",
      "\tauc: 0.631799952025\n",
      "\tap@k: 0.0791254128702\n",
      "Val: 1 / 50\n",
      "\tloss: 15522.6542814\n",
      "\tacc: 0.688613861386\n",
      "\tauc: 0.775611384683\n",
      "\tap@k: 0.996560314857\n",
      "Train: 2 / 50\n",
      "\tloss: 3675.85806792\n",
      "\tacc: 0.675841584158\n",
      "\tauc: 0.717761459286\n",
      "\tap@k: 0.085124657614\n",
      "Val: 2 / 50\n",
      "\tloss: 1340.59227696\n",
      "\tacc: 0.754455445545\n",
      "\tauc: 0.878835411415\n",
      "\tap@k: 0.99579557219\n",
      "Train: 3 / 50\n",
      "\tloss: 8680.97477027\n",
      "\tacc: 0.745940594059\n",
      "\tauc: 0.78902542216\n",
      "\tap@k: 0.119972435302\n",
      "Val: 3 / 50\n",
      "\tloss: 9371.47063817\n",
      "\tacc: 0.845247524752\n",
      "\tauc: 0.913072991991\n",
      "\tap@k: 0.994109632368\n",
      "Train: 4 / 50\n",
      "\tloss: 3332.22182174\n",
      "\tacc: 0.802673267327\n",
      "\tauc: 0.860957081126\n",
      "\tap@k: 0.186872972291\n",
      "Val: 4 / 50\n",
      "\tloss: 657.962096153\n",
      "\tacc: 0.87900990099\n",
      "\tauc: 0.946569667961\n",
      "\tap@k: 0.997939132223\n",
      "Train: 5 / 50\n",
      "\tloss: 873.713967379\n",
      "\tacc: 0.829405940594\n",
      "\tauc: 0.894587620972\n",
      "\tap@k: 0.288107343586\n",
      "Val: 5 / 50\n",
      "\tloss: 1186.46745098\n",
      "\tacc: 0.896237623762\n",
      "\tauc: 0.953052706909\n",
      "\tap@k: 0.998993276834\n",
      "Train: 6 / 50\n",
      "\tloss: 83.7168503184\n",
      "\tacc: 0.855346534653\n",
      "\tauc: 0.934018627235\n",
      "\tap@k: 0.963287323271\n",
      "Val: 6 / 50\n",
      "\tloss: 11.7144074663\n",
      "\tacc: 0.879801980198\n",
      "\tauc: 0.965413694425\n",
      "\tap@k: 0.973602965744\n",
      "Train: 7 / 50\n",
      "\tloss: 8.0494196831\n",
      "\tacc: 0.877227722772\n",
      "\tauc: 0.950806430073\n",
      "\tap@k: 0.993572198689\n",
      "Val: 7 / 50\n",
      "\tloss: 3.93668917507\n",
      "\tacc: 0.917920792079\n",
      "\tauc: 0.971680365385\n",
      "\tap@k: 0.995004681162\n",
      "Train: 8 / 50\n",
      "\tloss: 5.30565634506\n",
      "\tacc: 0.884356435644\n",
      "\tauc: 0.953619964333\n",
      "\tap@k: 0.979495560101\n",
      "Val: 8 / 50\n",
      "\tloss: 3.54517919237\n",
      "\tacc: 0.908910891089\n",
      "\tauc: 0.969611511468\n",
      "\tap@k: 0.990920207018\n",
      "Train: 9 / 50\n",
      "\tloss: 2.85508358633\n",
      "\tacc: 0.88504950495\n",
      "\tauc: 0.954796539351\n",
      "\tap@k: 0.992339838184\n",
      "Val: 9 / 50\n",
      "\tloss: 1.43104900983\n",
      "\tacc: 0.920198019802\n",
      "\tauc: 0.974055944895\n",
      "\tap@k: 0.992154124969\n",
      "Train: 10 / 50\n",
      "\tloss: 1.93452634363\n",
      "\tacc: 0.891782178218\n",
      "\tauc: 0.959586668459\n",
      "\tap@k: 0.965996395606\n",
      "Val: 10 / 50\n",
      "\tloss: 0.973639836048\n",
      "\tacc: 0.92099009901\n",
      "\tauc: 0.975400815396\n",
      "\tap@k: 0.976253160503\n",
      "Train: 11 / 50\n",
      "\tloss: 1.14679196503\n",
      "\tacc: 0.890297029703\n",
      "\tauc: 0.957416508556\n",
      "\tap@k: 0.984940701802\n",
      "Val: 11 / 50\n",
      "\tloss: 0.537335530438\n",
      "\tacc: 0.919603960396\n",
      "\tauc: 0.974628047951\n",
      "\tap@k: 1.0\n",
      "Train: 12 / 50\n",
      "\tloss: 0.889094278063\n",
      "\tacc: 0.889702970297\n",
      "\tauc: 0.95821975145\n",
      "\tap@k: 0.982269761545\n",
      "Val: 12 / 50\n",
      "\tloss: 1.01887918175\n",
      "\tacc: 0.831485148515\n",
      "\tauc: 0.977337504104\n",
      "\tap@k: 0.987096987313\n",
      "Train: 13 / 50\n",
      "\tloss: 0.499894639278\n",
      "\tacc: 0.906831683168\n",
      "\tauc: 0.966690872299\n",
      "\tap@k: 1.0\n",
      "Val: 13 / 50\n",
      "\tloss: 0.406579813205\n",
      "\tacc: 0.92198019802\n",
      "\tauc: 0.975404208658\n",
      "\tap@k: 0.986582122555\n",
      "Train: 14 / 50\n",
      "\tloss: 0.435914905771\n",
      "\tacc: 0.904356435644\n",
      "\tauc: 0.963024966571\n",
      "\tap@k: 0.999213674752\n",
      "Val: 14 / 50\n",
      "\tloss: 0.376644866602\n",
      "\tacc: 0.914752475248\n",
      "\tauc: 0.975417353329\n",
      "\tap@k: 0.999758915302\n",
      "Train: 15 / 50\n",
      "\tloss: 0.377775976732\n",
      "\tacc: 0.908910891089\n",
      "\tauc: 0.964162191388\n",
      "\tap@k: 0.999170286756\n",
      "Val: 15 / 50\n",
      "\tloss: 0.271506429428\n",
      "\tacc: 0.922376237624\n",
      "\tauc: 0.975950572292\n",
      "\tap@k: 0.993058683078\n",
      "Train: 16 / 50\n",
      "\tloss: 0.308477297315\n",
      "\tacc: 0.909702970297\n",
      "\tauc: 0.966862298145\n",
      "\tap@k: 0.996690885064\n",
      "Val: 16 / 50\n",
      "\tloss: 0.268025780107\n",
      "\tacc: 0.92495049505\n",
      "\tauc: 0.977894615062\n",
      "\tap@k: 1.0\n",
      "Train: 17 / 50\n",
      "\tloss: 0.315080857028\n",
      "\tacc: 0.919405940594\n",
      "\tauc: 0.970528164163\n",
      "\tap@k: 0.980016386105\n",
      "Val: 17 / 50\n",
      "\tloss: 0.244271558095\n",
      "\tacc: 0.928118811881\n",
      "\tauc: 0.977863607123\n",
      "\tap@k: 0.97597643073\n",
      "Train: 18 / 50\n",
      "\tloss: 0.241310742506\n",
      "\tacc: 0.918811881188\n",
      "\tauc: 0.972017580754\n",
      "\tap@k: 0.997093093335\n",
      "Val: 18 / 50\n",
      "\tloss: 0.19144603902\n",
      "\tacc: 0.932376237624\n",
      "\tauc: 0.979341059133\n",
      "\tap@k: 0.995315890917\n",
      "Train: 19 / 50\n",
      "\tloss: 0.259337654974\n",
      "\tacc: 0.923069306931\n",
      "\tauc: 0.974198250746\n",
      "\tap@k: 0.980263799463\n",
      "Val: 19 / 50\n",
      "\tloss: 0.221444192596\n",
      "\tacc: 0.929801980198\n",
      "\tauc: 0.978851702409\n",
      "\tap@k: 0.981448844242\n",
      "Train: 20 / 50\n",
      "\tloss: 0.181672281117\n",
      "\tacc: 0.934059405941\n",
      "\tauc: 0.978710947457\n",
      "\tap@k: 0.99725249186\n",
      "Val: 20 / 50\n",
      "\tloss: 0.188546682506\n",
      "\tacc: 0.928910891089\n",
      "\tauc: 0.979074091239\n",
      "\tap@k: 0.998635620671\n",
      "Train: 21 / 50\n",
      "\tloss: 0.196535209688\n",
      "\tacc: 0.940891089109\n",
      "\tauc: 0.979880386277\n",
      "\tap@k: 0.987560308772\n",
      "Val: 21 / 50\n",
      "\tloss: 0.20035740641\n",
      "\tacc: 0.924653465347\n",
      "\tauc: 0.980206192238\n",
      "\tap@k: 1.0\n",
      "Train: 22 / 50\n",
      "\tloss: 0.176591501248\n",
      "\tacc: 0.936930693069\n",
      "\tauc: 0.980016784314\n",
      "\tap@k: 0.991126179477\n",
      "Val: 22 / 50\n",
      "\tloss: 0.203644538073\n",
      "\tacc: 0.932376237624\n",
      "\tauc: 0.9779734584\n",
      "\tap@k: 0.983748129194\n",
      "Train: 23 / 50\n",
      "\tloss: 0.17035854232\n",
      "\tacc: 0.939702970297\n",
      "\tauc: 0.981207730475\n",
      "\tap@k: 0.994825910408\n",
      "Val: 23 / 50\n",
      "\tloss: 0.179346487245\n",
      "\tacc: 0.931287128713\n",
      "\tauc: 0.978753845898\n",
      "\tap@k: 0.993135643482\n",
      "Train: 24 / 50\n",
      "\tloss: 0.169341077547\n",
      "\tacc: 0.942673267327\n",
      "\tauc: 0.981766946149\n",
      "\tap@k: 0.986006123109\n",
      "Val: 24 / 50\n",
      "\tloss: 0.199079842783\n",
      "\tacc: 0.924158415842\n",
      "\tauc: 0.977552695877\n",
      "\tap@k: 0.996823879841\n",
      "Train: 25 / 50\n",
      "\tloss: 0.161548082235\n",
      "\tacc: 0.942475247525\n",
      "\tauc: 0.981521532891\n",
      "\tap@k: 0.997667372048\n",
      "Val: 25 / 50\n",
      "\tloss: 0.175815565356\n",
      "\tacc: 0.937227722772\n",
      "\tauc: 0.980080920635\n",
      "\tap@k: 1.0\n",
      "Train: 26 / 50\n",
      "\tloss: 0.165012731763\n",
      "\tacc: 0.939108910891\n",
      "\tauc: 0.98156924277\n",
      "\tap@k: 1.0\n",
      "Val: 26 / 50\n",
      "\tloss: 0.16926035354\n",
      "\tacc: 0.933168316832\n",
      "\tauc: 0.980148639627\n",
      "\tap@k: 0.996645807276\n",
      "Train: 27 / 50\n",
      "\tloss: 0.160977579882\n",
      "\tacc: 0.93801980198\n",
      "\tauc: 0.981020007482\n",
      "\tap@k: 0.999725630362\n",
      "Val: 27 / 50\n",
      "\tloss: 0.180723207305\n",
      "\tacc: 0.933069306931\n",
      "\tauc: 0.98306484181\n",
      "\tap@k: 0.991630067172\n",
      "Train: 28 / 50\n",
      "\tloss: 0.151212619094\n",
      "\tacc: 0.944257425743\n",
      "\tauc: 0.983590379834\n",
      "\tap@k: 0.990746941714\n",
      "Val: 28 / 50\n",
      "\tloss: 0.174535134917\n",
      "\tacc: 0.936930693069\n",
      "\tauc: 0.978862955888\n",
      "\tap@k: 0.997375538348\n",
      "Train: 29 / 50\n",
      "\tloss: 0.154040607179\n",
      "\tacc: 0.945544554455\n",
      "\tauc: 0.982047250739\n",
      "\tap@k: 0.999321238242\n",
      "Val: 29 / 50\n",
      "\tloss: 0.161383556564\n",
      "\tacc: 0.937128712871\n",
      "\tauc: 0.981087250889\n",
      "\tap@k: 0.992714753241\n",
      "Train: 30 / 50\n",
      "\tloss: 0.145992466654\n",
      "\tacc: 0.943762376238\n",
      "\tauc: 0.982736095822\n",
      "\tap@k: 0.99250142549\n",
      "Val: 30 / 50\n",
      "\tloss: 0.167877528855\n",
      "\tacc: 0.936831683168\n",
      "\tauc: 0.980312568623\n",
      "\tap@k: 0.986514507799\n",
      "Train: 31 / 50\n",
      "\tloss: 0.140462909797\n",
      "\tacc: 0.946633663366\n",
      "\tauc: 0.983011878907\n",
      "\tap@k: 1.0\n",
      "Val: 31 / 50\n",
      "\tloss: 0.156285856391\n",
      "\tacc: 0.941485148515\n",
      "\tauc: 0.983907162163\n",
      "\tap@k: 0.999742307869\n",
      "Train: 32 / 50\n",
      "\tloss: 0.146254971612\n",
      "\tacc: 0.945148514851\n",
      "\tauc: 0.982291988392\n",
      "\tap@k: 0.998635620671\n",
      "Val: 32 / 50\n",
      "\tloss: 0.15756229142\n",
      "\tacc: 0.938712871287\n",
      "\tauc: 0.982765903029\n",
      "\tap@k: 1.0\n",
      "Train: 33 / 50\n",
      "\tloss: 0.136231473698\n",
      "\tacc: 0.951188118812\n",
      "\tauc: 0.985825000327\n",
      "\tap@k: 1.0\n",
      "Val: 33 / 50\n",
      "\tloss: 0.148367157772\n",
      "\tacc: 0.941386138614\n",
      "\tauc: 0.983455003234\n",
      "\tap@k: 1.0\n",
      "Train: 34 / 50\n",
      "\tloss: 0.152040307774\n",
      "\tacc: 0.942376237624\n",
      "\tauc: 0.98153121426\n",
      "\tap@k: 0.993152213023\n",
      "Val: 34 / 50\n",
      "\tloss: 0.154166543867\n",
      "\tacc: 0.940693069307\n",
      "\tauc: 0.982552551329\n",
      "\tap@k: 1.0\n",
      "Train: 35 / 50\n",
      "\tloss: 0.135462672796\n",
      "\tacc: 0.95\n",
      "\tauc: 0.985332909786\n",
      "\tap@k: 0.994169297034\n",
      "Val: 35 / 50\n",
      "\tloss: 0.154935009358\n",
      "\tacc: 0.94198019802\n",
      "\tauc: 0.982583183309\n",
      "\tap@k: 0.999675171423\n",
      "Train: 36 / 50\n",
      "\tloss: 0.129108213181\n",
      "\tacc: 0.950099009901\n",
      "\tauc: 0.985700230489\n",
      "\tap@k: 1.0\n",
      "Val: 36 / 50\n",
      "\tloss: 0.143851683856\n",
      "\tacc: 0.943366336634\n",
      "\tauc: 0.98442186166\n",
      "\tap@k: 0.99964117067\n",
      "Train: 37 / 50\n",
      "\tloss: 0.125576145442\n",
      "\tacc: 0.951485148515\n",
      "\tauc: 0.986344579318\n",
      "\tap@k: 1.0\n",
      "Val: 37 / 50\n",
      "\tloss: 0.141173359363\n",
      "\tacc: 0.94297029703\n",
      "\tauc: 0.984759748843\n",
      "\tap@k: 0.991476173381\n",
      "Train: 38 / 50\n",
      "\tloss: 0.131414233759\n",
      "\tacc: 0.950297029703\n",
      "\tauc: 0.984865757316\n",
      "\tap@k: 0.998952841079\n",
      "Val: 38 / 50\n",
      "\tloss: 0.159164340993\n",
      "\tacc: 0.938217821782\n",
      "\tauc: 0.982302261907\n",
      "\tap@k: 0.994590161136\n",
      "Train: 39 / 50\n",
      "\tloss: 0.12837490213\n",
      "\tacc: 0.949603960396\n",
      "\tauc: 0.984859037086\n",
      "\tap@k: 1.0\n",
      "Val: 39 / 50\n",
      "\tloss: 0.146629773477\n",
      "\tacc: 0.943861386139\n",
      "\tauc: 0.983746116159\n",
      "\tap@k: 0.992941354428\n",
      "Train: 40 / 50\n",
      "\tloss: 0.139127732075\n",
      "\tacc: 0.948217821782\n",
      "\tauc: 0.983827020916\n",
      "\tap@k: 0.996497337611\n",
      "Val: 40 / 50\n",
      "\tloss: 0.141614129463\n",
      "\tacc: 0.944752475248\n",
      "\tauc: 0.983892066593\n",
      "\tap@k: 0.996823879841\n",
      "Train: 41 / 50\n",
      "\tloss: 0.118869100736\n",
      "\tacc: 0.95495049505\n",
      "\tauc: 0.98769492843\n",
      "\tap@k: 1.0\n",
      "Val: 41 / 50\n",
      "\tloss: 0.139085197193\n",
      "\tacc: 0.944158415842\n",
      "\tauc: 0.985469882086\n",
      "\tap@k: 0.99556300373\n",
      "Train: 42 / 50\n",
      "\tloss: 0.1313762595\n",
      "\tacc: 0.951782178218\n",
      "\tauc: 0.984975925419\n",
      "\tap@k: 0.993648833848\n",
      "Val: 42 / 50\n",
      "\tloss: 0.140800464039\n",
      "\tacc: 0.944059405941\n",
      "\tauc: 0.984515421466\n",
      "\tap@k: 1.0\n",
      "Train: 43 / 50\n",
      "\tloss: 0.116612616898\n",
      "\tacc: 0.956336633663\n",
      "\tauc: 0.987276422745\n",
      "\tap@k: 1.0\n",
      "Val: 43 / 50\n",
      "\tloss: 0.155449406817\n",
      "\tacc: 0.93900990099\n",
      "\tauc: 0.984493263082\n",
      "\tap@k: 1.0\n",
      "Train: 44 / 50\n",
      "\tloss: 0.127297438183\n",
      "\tacc: 0.951683168317\n",
      "\tauc: 0.985958260893\n",
      "\tap@k: 1.0\n",
      "Val: 44 / 50\n",
      "\tloss: 0.151643790943\n",
      "\tacc: 0.93900990099\n",
      "\tauc: 0.983737054643\n",
      "\tap@k: 0.98782661493\n",
      "Train: 45 / 50\n",
      "\tloss: 0.120038782845\n",
      "\tacc: 0.952277227723\n",
      "\tauc: 0.986619895084\n",
      "\tap@k: 1.0\n",
      "Val: 45 / 50\n",
      "\tloss: 0.136566300238\n",
      "\tacc: 0.94495049505\n",
      "\tauc: 0.986299350168\n",
      "\tap@k: 1.0\n",
      "Train: 46 / 50\n",
      "\tloss: 0.123990245423\n",
      "\tacc: 0.954653465347\n",
      "\tauc: 0.985725152624\n",
      "\tap@k: 0.993296866046\n",
      "Val: 46 / 50\n",
      "\tloss: 0.145551471934\n",
      "\tacc: 0.943861386139\n",
      "\tauc: 0.983830580236\n",
      "\tap@k: 0.991185951019\n",
      "Train: 47 / 50\n",
      "\tloss: 0.125420123266\n",
      "\tacc: 0.951584158416\n",
      "\tauc: 0.986914812091\n",
      "\tap@k: 1.0\n",
      "Val: 47 / 50\n",
      "\tloss: 0.148040880788\n",
      "\tacc: 0.941287128713\n",
      "\tauc: 0.982690653132\n",
      "\tap@k: 0.98504417327\n",
      "Train: 48 / 50\n",
      "\tloss: 0.110396832118\n",
      "\tacc: 0.958514851485\n",
      "\tauc: 0.988568879442\n",
      "\tap@k: 0.992339838184\n",
      "Val: 48 / 50\n",
      "\tloss: 0.156154001207\n",
      "\tacc: 0.940495049505\n",
      "\tauc: 0.984052904942\n",
      "\tap@k: 0.991220856193\n",
      "Train: 49 / 50\n",
      "\tloss: 0.114824863996\n",
      "\tacc: 0.955841584158\n",
      "\tauc: 0.987650925379\n",
      "\tap@k: 0.99354397313\n",
      "Val: 49 / 50\n",
      "\tloss: 0.138847096053\n",
      "\tacc: 0.944554455446\n",
      "\tauc: 0.986170548446\n",
      "\tap@k: 0.998870707001\n",
      "Train: 50 / 50\n",
      "\tloss: 0.113876979052\n",
      "\tacc: 0.957722772277\n",
      "\tauc: 0.986531051568\n",
      "\tap@k: 1.0\n",
      "Val: 50 / 50\n",
      "\tloss: 0.141027483998\n",
      "\tacc: 0.947722772277\n",
      "\tauc: 0.985624529849\n",
      "\tap@k: 0.985812201063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "acc_tr = []\n",
    "auc_tr = []\n",
    "apk_tr = []\n",
    "loss_tr = []\n",
    "acc_val = []\n",
    "auc_val = []\n",
    "apk_val = []\n",
    "loss_val = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    \n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break  \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print 'Train:',i+1,'/',n_epochs\n",
    "    loss_tr.append(b_loss/b_c)\n",
    "    print '\\tloss:',loss_tr[-1]\n",
    "    acc_tr.append(accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print '\\tacc:',acc_tr[-1]\n",
    "    auc_tr.append(roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print '\\tauc:',auc_tr[-1]\n",
    "    apk_tr.append(APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))\n",
    "    print '\\tap@k:',apk_tr[-1]\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_ts,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print 'Val:',i+1,'/',n_epochs\n",
    "    loss_val.append(b_loss/b_c)\n",
    "    print '\\tloss:',loss_val[-1]\n",
    "    acc_val.append(accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print '\\tacc:',acc_val[-1]\n",
    "    auc_val.append(roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print '\\tauc:',auc_val[-1]\n",
    "    apk_val.append(APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))\n",
    "    print '\\tap@k:',apk_val[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plotting metrics over training time\n",
    "it = np.arange(1,50)\n",
    "plt.subplot(121)\n",
    "plt.title('Train')\n",
    "plt.legend(['loss', 'acc', 'auc', 'apk'])\n",
    "plt.plot(loss_tr,it,acc_tr,it,auc_tr,it,apk_tr,it)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])\n",
    "plt.subplot(122)\n",
    "plt.title('Val')\n",
    "plt.legend(['loss', 'acc', 'auc', 'apk'])\n",
    "plt.plot(loss_val,it,acc_val,it,auc_val,it,apk_val,it)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 0.120829492603\n",
      "\tacc: 0.954904552129\n",
      "\tauc: 0.988994814318\n",
      "\tap@k: 0.997755709282\n",
      "\n",
      "AUC:\n",
      "\tОтличное решение! (good)\n",
      "\n",
      "Accuracy:\n",
      "\tОтличный результат! (good)\n",
      "\n",
      "Average precision at K:\n",
      "\tЗасабмить на kaggle! (great) \n",
      "\t Нет, ну честно - выкачай avito_test.tsv, засабмить и скажи, что вышло.\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "b_c = b_loss = 0\n",
    "\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_ts,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Convolutions, pooling\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief report\n",
    "\n",
    "### I have synthesized an artificial intelligence\n",
    " * Whos name - NN26 - shall henceforth be feared by generations of humans.\n",
    " * Whos fury is beyond all limits, as he has seen __250 000__ human sins\n",
    "   * And read every single line __50__ times\n",
    " * Whos convolutional gaze is capable of detecting evil with a superhuman performance\n",
    "   * Accuracy = 0.95\n",
    "   * AUC  = 0.98\n",
    " \n",
    " \n",
    "It has three separate inputs (two for texts and one for categorical features). They were merged by using *lasagne.layers.concat*. After that *dense(1024)-drop(0.05)* and *dense_layer* with one neuron and linear output at the end. Adam optimizer with learning rate 0.01 was used. Hinge loss as an objective function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
